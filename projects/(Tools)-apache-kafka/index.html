<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Apache kafka | Shubham Nagar </title> <meta name="author" content="Shubham Nagar"> <meta name="description" content="Kafka How and Why"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shubham184.github.io/projects/(Tools)-apache-kafka/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shubham</span> Nagar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Apache kafka</h1> <p class="post-description">Kafka How and Why</p> </header> <article> <h1 id="kafka-explained-simply">Kafka Explained Simply</h1> <h2 id="what-is-kafka">What is Kafka?</h2> <p>Imagine Kafka as a super-efficient post office for digital information. Here’s how it works:</p> <ol> <li> <p><strong>Sending Messages</strong>: Different people or computer systems can send messages (called “events” or “records” in Kafka).</p> </li> <li> <p><strong>Organizing Messages</strong>: Kafka organizes them into separate mailboxes (called “topics”). Each topic is for a specific type of message.</p> </li> <li> <p><strong>Storing Messages</strong>: Unlike a regular post office, Kafka keeps copies of all messages for a set time.</p> </li> <li> <p><strong>Delivering Messages</strong>: When someone wants to read messages, they can subscribe to a topic. Kafka will then deliver all messages from that topic to them.</p> </li> <li> <p><strong>Speed and Scale</strong>: Kafka can handle millions of messages per second, much faster than any human post office!</p> </li> </ol> <h2 id="simple-example-a-weather-app">Simple Example: A Weather App</h2> <p>Let’s say you’re building a weather app. Here’s how you might use Kafka:</p> <ol> <li> <strong>Weather Stations</strong> (Senders) send messages about current weather to Kafka.</li> <li>All these messages go into a “Weather Updates” topic (Mailbox) in Kafka.</li> <li>When someone opens your weather app (Receivers), it subscribes to the “Weather Updates” topic for their location.</li> <li>As new weather data arrives in Kafka, it’s immediately sent to all subscribed apps.</li> <li>If a user wants to see yesterday’s weather, the app can still get that information from Kafka.</li> </ol> <h2 id="how-does-kafka-achieve-this">How Does Kafka Achieve This?</h2> <p>Kafka uses several clever techniques to handle millions of messages efficiently:</p> <ol> <li> <p><strong>Multiple Post Offices (Distributed System)</strong>: Kafka uses many smaller post offices (called “brokers”) spread across different locations.</p> </li> <li> <p><strong>Organized Mailboxes (Partitioned Topics)</strong>: Each topic is divided into smaller sections called “partitions”, allowing multiple workers to process mail simultaneously.</p> </li> <li> <p><strong>Efficient Mail Sorting (Log-based Storage)</strong>: Kafka writes messages into a log file, like a giant list, making it fast to add new messages.</p> </li> <li> <p><strong>Quick Delivery (Zero-Copy Principle)</strong>: Kafka doesn’t make unnecessary copies when delivering messages.</p> </li> <li> <p><strong>Group Subscriptions (Consumer Groups)</strong>: Multiple readers can form a group to share the work of reading messages.</p> </li> <li> <p><strong>Keeping Backups (Replication)</strong>: Kafka makes copies of each message across different brokers for safety.</p> </li> <li> <p><strong>Efficient Record-Keeping (Offset Management)</strong>: Kafka keeps track of which messages each reader has already seen.</p> </li> <li> <p><strong>Batching for Efficiency</strong>: Kafka groups messages into batches for more efficient sending and receiving.</p> </li> <li> <p><strong>Smart Data Storage (Page Cache Usage)</strong>: Kafka uses the computer’s memory cleverly to store frequently accessed data.</p> </li> </ol> <p>By combining all these techniques, Kafka can handle an enormous amount of data very quickly and reliably, making it ideal for applications that need to process large volumes of real-time data.</p> <h2 id="a-overview-of-apache-kafka">a. Overview of Apache Kafka</h2> <p>Apache Kafka is a distributed event streaming platform capable of handling trillions of events a day. Initially developed by LinkedIn and later open-sourced, Kafka is designed for high-throughput, fault-tolerant, publish-subscribe messaging.</p> <p>Key features of Kafka include:</p> <ul> <li>Distributed architecture for scalability and fault-tolerance</li> <li>Persistent storage of streams of records</li> <li>High-throughput for both publishing and subscribing</li> <li>Streams processing capabilities</li> <li>Ecosystem of tools like Kafka Connect for data integration</li> </ul> <h2 id="b-why-kafka-was-chosen-for-globallogistics-supply-chain">b. Why Kafka was chosen for GlobalLogistics’ supply chain</h2> <p>GlobalLogistics chose Kafka for several reasons specific to their supply chain context:</p> <ol> <li> <p><strong>High-volume data handling</strong>: GlobalLogistics processes over 500 million events daily, including inventory updates, shipment tracking, and order statuses. Kafka’s ability to handle high-throughput messaging was crucial.</p> </li> <li> <p><strong>Real-time capabilities</strong>: The supply chain industry demands real-time visibility. Kafka’s low-latency data streaming enables GlobalLogistics to achieve near real-time updates across their entire network.</p> </li> <li> <p><strong>Decoupling of systems</strong>: With operations in 50+ countries, GlobalLogistics needed to integrate diverse systems. Kafka’s publish-subscribe model allowed for loose coupling between data producers and consumers.</p> </li> <li> <p><strong>Scalability</strong>: As GlobalLogistics expands to new markets, Kafka’s distributed nature ensures the data infrastructure can scale horizontally with ease.</p> </li> <li> <p><strong>Data persistence</strong>: Kafka’s log-based persistence model allows for replay of data streams, crucial for auditing and recovery in supply chain operations.</p> </li> <li> <p><strong>Ecosystem integration</strong>: Kafka’s rich ecosystem, including Kafka Connect, facilitates integration with various data sources and sinks common in supply chain management.</p> </li> </ol> <h2 id="c-implementation-in-the-globallogistics-project">c. Implementation in the GlobalLogistics project</h2> <p>The implementation of Kafka in GlobalLogistics’ supply chain involved several key steps:</p> <ol> <li> <p><strong>Cluster Setup</strong>:</p> <ul> <li>Deployed a 15-node Kafka cluster across three data centers for high availability.</li> <li>Used Kafka version 2.8.0 for its improved Raft-based metadata quorum.</li> </ul> </li> <li> <p><strong>Topic Design</strong>:</p> <ul> <li>Created topic “inventory-updates” for real-time inventory changes.</li> <li>Designed “shipment-tracking” topic for live shipment status updates.</li> <li>Implemented “order-events” topic for order lifecycle events.</li> </ul> </li> <li> <p><strong>Producer Development</strong>:</p> <ul> <li>Developed Kafka producers for key systems: <ul> <li>WMS (Manhattan Associates) for inventory updates</li> <li>TMS (BluJay Solutions) for shipment tracking</li> <li>OMS (IBM Sterling) for order events</li> </ul> </li> <li>Implemented Avro schemas for each event type, managed via Confluent Schema Registry.</li> </ul> </li> <li> <p><strong>Consumer Applications</strong>:</p> <ul> <li>Developed multiple consumer applications: <ul> <li>Real-time dashboard (using Kafka Streams and React)</li> <li>Predictive analytics engine (using Kafka-Python and TensorFlow)</li> <li>Alerting system (using Kafka Connect and Elastic Stack)</li> </ul> </li> </ul> </li> <li> <p><strong>Data Integration</strong>:</p> <ul> <li>Utilized Kafka Connect for integrating with: <ul> <li>Oracle database for master data</li> <li>Elasticsearch for search and analytics</li> <li>Snowflake for data warehousing</li> </ul> </li> </ul> </li> <li> <p><strong>Monitoring and Management</strong>:</p> <ul> <li>Implemented Confluent Control Center for cluster management</li> <li>Set up Prometheus and Grafana for real-time monitoring and alerting</li> </ul> </li> </ol> <h2 id="d-specific-challenges-addressed">d. Specific challenges addressed</h2> <ol> <li> <p><strong>Data Consistency</strong>: Ensured consistent data views across different systems by using Kafka as the single source of truth.</p> </li> <li> <p><strong>Latency Reduction</strong>: Reduced end-to-end latency from 2 minutes to under 500 milliseconds for critical updates like inventory changes.</p> </li> <li> <p><strong>System Integration</strong>: Integrated 20+ disparate systems across the supply chain, replacing point-to-point integrations with a hub-and-spoke model.</p> </li> <li> <p><strong>Scalability</strong>: Handled a 3x increase in data volume during the holiday season without performance degradation.</p> </li> <li> <p><strong>Real-time Analytics</strong>: Enabled real-time analytics for instant decision-making, particularly for inventory optimization and shipment routing.</p> </li> </ol> <h2 id="e-benefits-and-results-achieved">e. Benefits and results achieved</h2> <ol> <li> <p><strong>Improved Operational Efficiency</strong>:</p> <ul> <li>Reduced order fulfillment time by 28% (from 25 hours to 18 hours on average)</li> <li>Increased inventory turns by 15% due to better visibility and forecasting</li> </ul> </li> <li> <p><strong>Enhanced Customer Satisfaction</strong>:</p> <ul> <li>Improved on-time delivery rate from 82% to 95%</li> <li>Reduced customer complaints related to shipment updates by 40%</li> </ul> </li> <li> <p><strong>Cost Savings</strong>:</p> <ul> <li>Decreased integration development time by 40%, saving approximately $2 million annually in IT costs</li> <li>Reduced infrastructure costs by 25% compared to the previous system</li> </ul> </li> <li> <p><strong>Improved Decision Making</strong>:</p> <ul> <li>Enabled real-time analytics, leading to a 20% reduction in stockouts</li> <li>Facilitated predictive maintenance, reducing vehicle downtime by 15%</li> </ul> </li> <li> <p><strong>Scalability and Performance</strong>:</p> <ul> <li>Successfully handled a 300% increase in data volume during peak seasons</li> <li>Achieved a throughput of over 1 million events per second during peak times</li> </ul> </li> </ol> <h2 id="f-limitations-and-future-considerations">f. Limitations and future considerations</h2> <ol> <li> <p><strong>Schema Evolution</strong>: Managing schema changes across numerous producers and consumers remains challenging. Considering implementing a more robust schema evolution strategy.</p> </li> <li> <p><strong>Data Quality</strong>: While Kafka ensures delivery, it doesn’t inherently manage data quality. Planning to implement data quality checks at the producer level.</p> </li> <li> <p><strong>Multi-Region Replication</strong>: Current setup doesn’t support global failover. Exploring Kafka’s MirrorMaker 2.0 for multi-region replication.</p> </li> <li> <p><strong>Security</strong>: While basic security measures are in place, considering implementing more advanced features like SASL/SCRAM authentication and encryption of data at rest.</p> </li> <li> <p><strong>Cost of Scale</strong>: As data volumes grow, the cost of maintaining a large Kafka cluster is increasing. Exploring cloud-based Kafka solutions for potential cost optimization.</p> </li> </ol> <h2 id="g-real-world-examples-of-kafka-usage-in-the-project">g. Real-world examples of Kafka usage in the project</h2> <ol> <li> <p><strong>Real-time Inventory Management</strong>:</p> <ul> <li>Use Case: Maintaining accurate inventory levels across 100+ warehouses globally.</li> <li>Implementation: Each warehouse management system publishes inventory changes to the “inventory-updates” Kafka topic. A Kafka Streams application consumes these events, aggregates them, and updates a real-time inventory dashboard.</li> <li>Result: Reduced inventory discrepancies by 35%, leading to better stock management and fewer stockouts.</li> </ul> </li> <li> <p><strong>Predictive Shipment Delay Alerts</strong>:</p> <ul> <li>Use Case: Proactively alerting customers about potential shipment delays.</li> <li>Implementation: GPS data from delivery vehicles is published to the “shipment-tracking” topic. A machine learning model consumes this data, along with historical traffic and weather data, to predict potential delays. If a delay is predicted, an event is published to an “shipment-alerts” topic, which triggers customer notifications.</li> <li>Result: Customer satisfaction scores related to shipment updates improved by 25%.</li> </ul> </li> <li> <p><strong>Dynamic Route Optimization</strong>:</p> <ul> <li>Use Case: Optimizing delivery routes in real-time based on traffic and order data.</li> <li>Implementation: Traffic data is streamed to a “traffic-conditions” topic, while new orders are published to the “order-events” topic. A Kafka Streams application joins these streams with the “shipment-tracking” topic to continuously optimize routes.</li> <li>Result: Achieved a 12% reduction in fuel costs and improved on-time delivery rates.</li> </ul> </li> <li> <p><strong>Supply Chain Anomaly Detection</strong>:</p> <ul> <li>Use Case: Identifying unusual patterns in supply chain operations that could indicate issues.</li> <li>Implementation: Various operational metrics are published to different Kafka topics. A Kafka Streams application applies statistical analysis in real-time to detect anomalies. When detected, events are published to an “anomaly-alerts” topic.</li> <li>Result: Early detection of supply chain disruptions improved by 40%, allowing for faster response to potential issues.</li> </ul> </li> <li> <p><strong>Real-time Performance Dashboards</strong>:</p> <ul> <li>Use Case: Providing executives with real-time visibility into global operations.</li> <li>Implementation: Key performance indicators from various systems are published to Kafka topics. A Kafka Streams application aggregates this data and pushes updates to a WebSocket server, which feeds into a real-time dashboard built with React.</li> <li>Result: Reduced decision-making time for critical operations from hours to minutes, improving overall supply chain agility.</li> </ul> </li> </ol> <p>These real-world examples demonstrate how Kafka has become the backbone of GlobalLogistics’ data infrastructure, enabling real-time, data-driven operations across their global supply chain.</p> <h1 id="10-real-life-kafka-use-cases-in-the-telecom-industry">10 Real-Life Kafka Use Cases in the Telecom Industry</h1> <ol> <li> <p><strong>Real-time Network Performance Monitoring</strong></p> <ul> <li>Use Case: A major telecom provider will implement Kafka to monitor network performance across its entire infrastructure in real-time.</li> <li>Implementation: Network devices will publish performance metrics (e.g., latency, packet loss, throughput) to Kafka topics. A Kafka Streams application will process these metrics in real-time, detecting anomalies and triggering alerts.</li> <li>Expected Result: The provider will reduce network downtime by 30% through proactive issue detection and resolution.</li> </ul> </li> <li> <p><strong>Customer Experience Management</strong></p> <ul> <li>Use Case: A telecom company will use Kafka to create a 360-degree view of customer interactions across all channels (mobile app, website, call center).</li> <li>Implementation: Each customer touchpoint will publish event data to Kafka topics. A centralized application will consume these events, updating a real-time customer profile and triggering personalized actions.</li> <li>Expected Result: The company will see a 25% increase in customer satisfaction scores due to more personalized and timely interactions.</li> </ul> </li> <li> <p><strong>Fraud Detection in Call and Data Usage</strong></p> <ul> <li>Use Case: A telecom operator will implement a real-time fraud detection system using Kafka.</li> <li>Implementation: Call Detail Records (CDRs) and data usage logs will be streamed to Kafka topics. A machine learning model will consume these streams, analyzing patterns in real-time to detect potential fraud.</li> <li>Expected Result: The operator will reduce fraudulent activity by 40%, saving millions in revenue leakage.</li> </ul> </li> <li> <p><strong>Dynamic Resource Allocation in 5G Networks</strong></p> <ul> <li>Use Case: A 5G network provider will use Kafka for dynamic resource allocation based on real-time demand.</li> <li>Implementation: 5G base stations will publish current load and capacity data to Kafka topics. A Kafka Streams application will process this data, making real-time decisions on resource allocation.</li> <li>Expected Result: The provider will improve network efficiency by 35%, enhancing user experience during peak usage times.</li> </ul> </li> <li> <p><strong>IoT Device Management for Telecom Infrastructure</strong></p> <ul> <li>Use Case: A telecom infrastructure company will implement Kafka to manage millions of IoT devices deployed for network monitoring and maintenance.</li> <li>Implementation: IoT devices will publish status updates and sensor data to Kafka topics. A central management system will consume these events, updating device statuses and triggering maintenance workflows.</li> <li>Expected Result: The company will reduce field service visits by 50% through predictive maintenance and remote diagnostics.</li> </ul> </li> <li> <p><strong>Real-time Billing and Usage Monitoring</strong></p> <ul> <li>Use Case: A mobile network operator will use Kafka to implement real-time billing and usage monitoring for its prepaid customers.</li> <li>Implementation: Usage data (calls, SMS, data) will be published to Kafka topics in real-time. A billing application will consume these events, updating customer balances and triggering notifications for low balance or high usage.</li> <li>Expected Result: The operator will see a 60% reduction in billing-related customer complaints and a 20% increase in timely balance top-ups.</li> </ul> </li> <li> <p><strong>Network Capacity Planning</strong></p> <ul> <li>Use Case: A telecom provider will implement Kafka for data-driven network capacity planning.</li> <li>Implementation: Network usage data from various sources will be aggregated in Kafka topics. A big data analytics platform will consume this data, generating forecasts and recommendations for network upgrades.</li> <li>Expected Result: The provider will optimize its capital expenditure, reducing unnecessary upgrades by 25% while improving network quality.</li> </ul> </li> <li> <p><strong>Voice Quality Monitoring and Improvement</strong></p> <ul> <li>Use Case: A VoIP service provider will use Kafka to monitor and improve voice call quality in real-time.</li> <li>Implementation: Call quality metrics (jitter, packet loss, MOS scores) will be published to Kafka topics for each ongoing call. A real-time analytics engine will process these streams, dynamically adjusting network parameters to optimize call quality.</li> <li>Expected Result: The provider will see a 30% improvement in average call quality scores and a 40% reduction in dropped calls.</li> </ul> </li> <li> <p><strong>Subscriber Churn Prediction and Prevention</strong></p> <ul> <li>Use Case: A telecom operator will implement a real-time churn prediction system using Kafka.</li> <li>Implementation: Customer interaction data, usage patterns, and network experience metrics will be streamed to Kafka topics. A machine learning model will consume these streams, generating real-time churn risk scores and triggering retention actions.</li> <li>Expected Result: The operator will reduce customer churn by 20% through timely and personalized retention efforts.</li> </ul> </li> <li> <p><strong>Cross-Sell and Upsell Recommendation Engine</strong></p> <ul> <li>Use Case: A telecom service provider will implement a real-time recommendation engine using Kafka to drive cross-sell and upsell opportunities.</li> <li>Implementation: Customer usage data, browsing behavior, and interaction history will be published to Kafka topics. A recommendation engine will consume these streams, generating personalized offers in real-time.</li> <li>Expected Result: The provider will see a 40% increase in successful upsells and a 30% improvement in average revenue per user (ARPU).</li> </ul> </li> </ol> <p>These use cases demonstrate the versatility and power of Kafka in addressing various challenges and opportunities in the telecom industry. By leveraging Kafka’s real-time data streaming capabilities, telecom companies can enhance operational efficiency, improve customer experience, and drive business growth in an increasingly competitive market.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Shubham Nagar. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"projects",description:"A growing collection of my data projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-migrating-from-tableau-to-power-bi-a-comprehensive-guide",title:'Migrating from Tableau to Power BI: A Comprehensive Guide <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/mind-and-machine/migrating-from-tableau-to-power-bi-a-comprehensive-guide-b6e4929e1ea3?source=rss-5ad90eb46828------2","_blank")}},{id:"post-discovering-anomalies-with-mad-the-secret-sauce-for-accurate-data-analysis",title:'Discovering Anomalies with MAD: The Secret Sauce for Accurate Data Analysis <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/mind-and-machine/discovering-anomalies-with-mad-the-secret-sauce-for-accurate-data-analysis-ed1c7909e2bf?source=rss-5ad90eb46828------2","_blank")}},{id:"post-harnessing-agentic-rag-and-graph-based-metadata-filtering-for-enhanced-information-retrieval",title:'Harnessing Agentic RAG and Graph-Based Metadata Filtering for Enhanced Information Retrieval <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/mind-and-machine/harnessing-agentic-rag-and-graph-based-metadata-filtering-for-enhanced-information-retrieval-5e4fc88dcdc0?source=rss-5ad90eb46828------2","_blank")}},{id:"post-the-critical-role-of-red-teaming-in-ai-development",title:'The Critical Role of Red Teaming in AI Development <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/mind-and-machine/the-critical-role-of-red-teaming-in-ai-development-8a1b393cfc51?source=rss-5ad90eb46828------2","_blank")}},{id:"post-who-am-i",title:'Who am I? <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@mail.shubhamnagar/who-am-i-f6810254e1ba?source=rss-5ad90eb46828------2","_blank")}},{id:"post-trust-or-distrust-bridging-ai-and-blockchain",title:'Trust or Distrust: Bridging AI and Blockchain <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/mind-and-machine/trust-or-distrust-bridging-ai-and-blockchain-659c5760ef6?source=rss-5ad90eb46828------2","_blank")}},{id:"post-ai-technologies-in-the-new-era-catalysts-or-hindrances-for-the-rpa-industry",title:'AI Technologies in the New Era: Catalysts or Hindrances for the RPA Industry?... <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@mail.shubhamnagar/ai-technologies-in-the-new-era-catalysts-or-hindrances-for-the-rpa-industry-e3c083950ced?source=rss-5ad90eb46828------2","_blank")}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-advanced-supply-chain-analytics-platform-optimizing-global-logistics-with-real-time-data",title:"Advanced Supply Chain Analytics Platform - Optimizing Global Logistics with Real-time Data",description:"Developed a cutting-edge supply chain analytics platform integrating real-time IoT data, predictive analytics, and interactive visualizations to optimize global logistics operations.",section:"Projects",handler:()=>{window.location.href="/projects/(Analytics)-advanced-data-visualization-project/"}},{id:"projects-data-modeling-for-e-commerce-reducing-redundancy-and-improving-customer-insights",title:"Data Modeling for E-commerce - Reducing Redundancy and Improving Customer Insights",description:"A comprehensive overhaul of an e-commerce platform's data model, resulting in 40% reduced data redundancy and 25% improved customer segmentation accuracy.",section:"Projects",handler:()=>{window.location.href="/projects/(Analytics)-ecommerce-data-modeling/"}},{id:"projects-snowflake-data-warehousing-project-optimizing-analytics-for-scale",title:"Snowflake Data Warehousing Project - Optimizing Analytics for Scale",description:"Implemented a Snowflake data warehouse solution, achieving a 40% reduction in query times and enabling real-time analytics across diverse data sources for a rapidly growing e-commerce company.",section:"Projects",handler:()=>{window.location.href="/projects/(Analytics)-snowflake-project/"}},{id:"projects-data-architecture-strategy-data-silos-ai-implementaion",title:"Data Architecture Strategy - Data Silos - AI Implementaion",description:"Enterprise Data Architect Case Study - Enabling AI Integration in Global Logistics",section:"Projects",handler:()=>{window.location.href="/projects/(DA)-AI-Implementaion/"}},{id:"projects-comprehensive-data-architecture-strategy-for-global-supply-chain-and-logistics",title:"Comprehensive Data Architecture Strategy for Global Supply Chain and Logistics",description:"Designed and implemented a comprehensive data architecture strategy for a global supply chain and logistics customer, improving data accessibility by 35% and streamlining reporting processes across their international operations.",section:"Projects",handler:()=>{window.location.href="/projects/(DA)-comprehensive-data-architecture-strategy/"}},{id:"projects-data-lake-to-data-fabric-modernizing-data-architecture-for-a-multinational-corporation",title:"Data Lake to Data Fabric - Modernizing Data Architecture for a Multinational Corporation...",description:"Transitioning from a traditional data lake to a data fabric approach, improving data accessibility, governance, and analytics agility",section:"Projects",handler:()=>{window.location.href="/projects/(DA)-data-fabric-case-study/"}},{id:"projects-data-pipeline-management-with-apache-spark-and-talend",title:"Data Pipeline Management with Apache Spark and Talend",description:"Designed and managed data pipelines for data validation, transformation, and cleaning using Apache Spark and Talend, ensuring the highest quality and reliability of data across the organization.",section:"Projects",handler:()=>{window.location.href="/projects/(DA)-data-pipeline-management/"}},{id:"projects-initiating-supply-chain-transparency-with-mdm",title:"Initiating Supply Chain Transparency with MDM",description:"EcoTrace Solutions - Initiating Supply Chain Transparency with MDM",section:"Projects",handler:()=>{window.location.href="/projects/(DG)-MDM/"}},{id:"projects-enhancing-metadata-management-using-collibra-business-glossary",title:"Enhancing Metadata Management Using Collibra Business Glossary",description:"Implemented a robust metadata management system using Collibra and integrated it with Mega HOPEX for improved data governance and consistency across the organization.",section:"Projects",handler:()=>{window.location.href="/projects/(DG)-collibra-metadata-case-study/"}},{id:"projects-revolutionizing-supply-chain-data-governance-at-globallogistics",title:"Revolutionizing Supply Chain Data Governance at GlobalLogistics",description:"Implementing enterprise-wide data standards and quality measures to streamline global logistics operations",section:"Projects",handler:()=>{window.location.href="/projects/(DG)-data-governance-collibra/"}},{id:"projects-pioneering-data-governance-in-fashion-supply-chain-transparency",title:"Pioneering Data Governance in Fashion Supply Chain Transparency",description:"Pioneering Data Governance in Fashion Supply Chain Transparency",section:"Projects",handler:()=>{window.location.href="/projects/(DG)-data-governance-exp/"}},{id:"projects-implementing-data-governance-a-journey-from-silos-to-standardization",title:"Implementing Data Governance - A Journey from Silos to Standardization",description:"Transforming organizational data management through strategic governance initiatives",section:"Projects",handler:()=>{window.location.href="/projects/(DG)-implementing-data-governance/"}},{id:"projects-enhancing-regulatory-compliance-and-data-governance-in-global-supply-chain-operations",title:"Enhancing Regulatory Compliance and Data Governance in Global Supply Chain Operations",description:"A case study on implementing Collibra and Mega HOPEX to streamline compliance and data management for GlobalLogistics",section:"Projects",handler:()=>{window.location.href="/projects/(DG)-regulatory-compliance/"}},{id:"projects-data-integration-project-telco-colibra",title:"Data Integration Project (Telco) - Colibra",description:"Implemented a comprehensive data integration solution for a global logistics company using Collibra and Mega HOPEX, resulting in improved operational efficiency and data-driven decision making.",section:"Projects",handler:()=>{window.location.href="/projects/(DI)-colibra-data-integration-telco/"}},{id:"projects-data-integration-project-supply-chain-colibra",title:"Data Integration Project (Supply Chain) - Colibra",description:"Implemented a comprehensive data integration solution for a global logistics company using Collibra and Mega HOPEX, resulting in improved operational efficiency and data-driven decision making.",section:"Projects",handler:()=>{window.location.href="/projects/(DI)-colibra-data-integration/"}},{id:"projects-esg-compliance-regional-telecom",title:"ESG Compliance - Regional Telecom",description:"A comprehensive data architecture overhaul to meet emerging ESG standards and optimize telecom operations in Belgium",section:"Projects",handler:()=>{window.location.href="/projects/(DI)-esg-telco/"}},{id:"projects-esg-compliance-supply-chain",title:"ESG Compliance - Supply Chain",description:"A comprehensive data architecture overhaul to meet emerging ESG standards and optimize supply chain operations",section:"Projects",handler:()=>{window.location.href="/projects/(DI)-esg/"}},{id:"projects-modernizing-supply-chain-integration-at-globallogistics",title:"Modernizing Supply Chain Integration at GlobalLogistics",description:"Implementing cutting-edge integration technologies to streamline global supply chain operations",section:"Projects",handler:()=>{window.location.href="/projects/(DI)-modern-supply-chain/"}},{id:"projects-modernizing-telco-integration-at-belgicom",title:"Modernizing Telco Integration at BelgiCom",description:"Proposed implementation of cutting-edge integration technologies to streamline telecom operations",section:"Projects",handler:()=>{window.location.href="/projects/(DI)-modern-telco-integration/"}},{id:"projects-streamlining-supply-chain-data-architecture-at-globallogistics",title:"Streamlining Supply Chain Data Architecture at GlobalLogistics",description:"A comprehensive data integration project that revolutionized GlobalLogistics' supply chain operations",section:"Projects",handler:()=>{window.location.href="/projects/(DI)-streamlining-supply-chain-architecture/"}},{id:"projects-scalable-real-time-data-platform-for-e-commerce",title:"Scalable Real-Time Data Platform for E-Commerce",description:"Implemented a high-performance data platform using API-first design, Kafka, FastAPI, and Airflow for an e-commerce giant, enabling real-time personalization and handling over 100,000 requests per second.",section:"Projects",handler:()=>{window.location.href="/projects/(Messaging)-realtime-data-platform/"}},{id:"projects-databricks",title:"Databricks",description:"Databricks Supply Chain Integration - A Case Study",section:"Projects",handler:()=>{window.location.href="/projects/(Tools)-Databricks/"}},{id:"projects-snowflake",title:"Snowflake",description:"Snowflake Supply Chain Integration - A Case Study",section:"Projects",handler:()=>{window.location.href="/projects/(Tools)-Snowflake/"}},{id:"projects-apache-kafka",title:"Apache kafka",description:"Kafka How and Why",section:"Projects",handler:()=>{window.location.href="/projects/(Tools)-apache-kafka/"}},{id:"projects-apache-spark",title:"Apache spark",description:"Spark How and Why",section:"Projects",handler:()=>{window.location.href="/projects/(Tools)-apche-spark/"}},{id:"projects-aws-sqs",title:"AWS SQS",description:"SQS How and Why",section:"Projects",handler:()=>{window.location.href="/projects/(Tools)-aws-sqs/"}},{id:"projects-how-collibra-and-leanix-complement-each-other-in-telecom",title:"How Collibra and LeanIX Complement Each Other in Telecom",description:"How Collibra and LeanIX Complement Each Other in Telecom",section:"Projects",handler:()=>{window.location.href="/projects/(Tools)-collibra-plus-leanix/"}},{id:"projects-collibra",title:"Collibra",description:"Collibra Explained Simply",section:"Projects",handler:()=>{window.location.href="/projects/(Tools)-collibra/"}},{id:"projects-sap-leanix",title:"SAP LeanIX",description:"LeanIX Explained Simply",section:"Projects",handler:()=>{window.location.href="/projects/(Tools)-leanix/"}},{id:"projects-mulesoft",title:"Mulesoft",description:"A deep dive into how MuleSoft Anypoint Platform transformed global supply chain operations",section:"Projects",handler:()=>{window.location.href="/projects/(Tools)-mulesoft/"}},{id:"projects-sqs-vs-kafka",title:"SQS vs Kafka",description:"Complementing Kafka with SQS",section:"Projects",handler:()=>{window.location.href="/projects/(Tools)-sqs-kafka/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6D%61%69%6C.%73%68%75%62%68%61%6D%6E%61%67%61%72@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/shubham184","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/shubham-nagar-222497151","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>