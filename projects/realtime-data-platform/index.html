<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Scalable Real-Time Data Platform for E-Commerce | Shubham Nagar </title> <meta name="author" content="Shubham Nagar"> <meta name="description" content="Implemented a high-performance data platform using API-first design, Kafka, FastAPI, and Airflow for an e-commerce giant, enabling real-time personalization and handling over 100,000 requests per second."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shubham184.github.io/projects/realtime-data-platform/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shubham</span> Nagar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Scalable Real-Time Data Platform for E-Commerce</h1> <p class="post-description">Implemented a high-performance data platform using API-first design, Kafka, FastAPI, and Airflow for an e-commerce giant, enabling real-time personalization and handling over 100,000 requests per second.</p> </header> <article> <h2 id="situation">Situation</h2> <p>As the lead Enterprise Data Architect at GlobalShop, a rapidly growing e-commerce platform with over 50 million active users, I faced several critical challenges:</p> <ol> <li> <p><strong>Data Processing Bottlenecks</strong>: Our legacy batch-oriented data processing systems struggled to handle the increasing volume of user interactions, product updates, and transaction data. This led to delays in updating product recommendations and personalized content, affecting user experience and sales conversion rates.</p> <p><em>Example</em>: During our summer sale, it took up to 4 hours to update product recommendations, resulting in outdated suggestions and missed cross-selling opportunities.</p> </li> <li> <p><strong>API Integration Complexity</strong>: The existing monolithic architecture made it difficult to integrate new services and third-party APIs. This slowed down the development of new features and hindered our ability to adapt to changing market demands.</p> <p><em>Example</em>: Integrating a new payment gateway took 3 months due to the tight coupling of our systems, delaying our expansion into a new market.</p> </li> <li> <p><strong>Scalability Issues</strong>: During peak shopping periods like Black Friday, our systems experienced significant slowdowns and occasional outages, unable to handle the surge in traffic and data processing requirements.</p> <p><em>Example</em>: Last Black Friday, our site experienced a 30-minute outage, resulting in an estimated $2 million in lost sales.</p> </li> <li> <p><strong>Real-Time Analytics Gap</strong>: Our marketing and product teams lacked access to real-time insights, hampering their ability to make data-driven decisions and respond quickly to emerging trends or issues.</p> <p><em>Example</em>: A pricing error on a popular product went unnoticed for 2 hours, leading to a significant loss in profit margin.</p> </li> <li> <p><strong>Inconsistent Data Models</strong>: Different teams were using inconsistent data models and schemas, leading to data quality issues and making it challenging to create a unified view of our customers and products.</p> <p><em>Example</em>: Customer segmentation efforts were hampered by inconsistent definitions of “active user” across different departments, leading to misaligned marketing campaigns.</p> </li> </ol> <h2 id="task">Task</h2> <p>As the lead data architect, my primary responsibilities were to:</p> <ol> <li>Design and implement a scalable, real-time data platform that could handle our current and future data processing needs.</li> <li>Develop an API-first architecture to simplify integration and enable rapid development of new features.</li> <li>Implement a robust event streaming solution to enable real-time data processing and analytics.</li> <li>Create a flexible and scalable microservices architecture to replace our monolithic system.</li> <li>Establish a unified data model and schema management system across the organization.</li> <li>Orchestrate complex data pipelines to ensure data consistency and timely updates across all systems.</li> </ol> <h2 id="action">Action</h2> <p>To address these challenges, I led a team of data engineers and software developers in implementing a comprehensive solution. Here’s a detailed account of the steps we took:</p> <h3 id="1-implementing-api-first-design-principles">1. Implementing API-First Design Principles</h3> <p>I introduced an API-first approach to our development process, which involved:</p> <ul> <li> <p><strong>API Design Workshops</strong>: I organized weekly workshops with product managers, developers, and data scientists to design and refine our API specifications before implementation.</p> <p><em>Example</em>: In one workshop, we designed the Product Catalog API, defining endpoints for product search, filtering, and detailed product information. We used user stories to guide the design, such as “As a mobile app user, I want to search for products by category and filter by price range.”</p> </li> <li> <p><strong>OpenAPI (Swagger) Specification</strong>: We used OpenAPI 3.0 to create detailed, standardized API documentation for all our services.</p> <p><em>Example</em>: For our User Profile API, we created an OpenAPI spec that included endpoints for user registration, profile updates, and preference management. Here’s a snippet:</p> <div class="language-yaml highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="na">paths</span><span class="pi">:</span>
  <span class="s">/users/{userId}/preferences</span><span class="err">:</span>
    <span class="na">get</span><span class="pi">:</span>
      <span class="na">summary</span><span class="pi">:</span> <span class="s">Retrieve user preferences</span>
      <span class="na">parameters</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">userId</span>
          <span class="na">in</span><span class="pi">:</span> <span class="s">path</span>
          <span class="na">required</span><span class="pi">:</span> <span class="kc">true</span>
          <span class="na">schema</span><span class="pi">:</span>
            <span class="na">type</span><span class="pi">:</span> <span class="s">string</span>
    <span class="na">put</span><span class="pi">:</span>
      <span class="na">summary</span><span class="pi">:</span> <span class="s">Update user preferences</span>
      <span class="na">requestBody</span><span class="pi">:</span>
        <span class="na">content</span><span class="pi">:</span>
          <span class="na">application/json</span><span class="pi">:</span>
            <span class="na">schema</span><span class="pi">:</span>
              <span class="na">$ref</span><span class="pi">:</span> <span class="s2">"</span><span class="s">#/components/schemas/UserPreferences"</span>
</code></pre></div> </div> </li> <li> <p><strong>API Gateway</strong>: Implemented Kong as our API gateway to manage, secure, and monitor all API traffic.</p> <p><em>Example</em>: We configured Kong to handle rate limiting, caching, and authentication for our APIs. For the Product Catalog API, we set a rate limit of 1000 requests per minute per API key and cached frequent product queries for 5 minutes to reduce database load.</p> </li> <li> <p><strong>Contract Testing</strong>: Introduced Pact for contract testing between service consumers and providers, ensuring API compatibility.</p> <p><em>Example</em>: We created Pact contracts between our mobile app (consumer) and the Product Catalog API (provider). This allowed us to catch breaking changes early, such as when a required field was accidentally removed from the product search response.</p> </li> </ul> <h3 id="2-using-kafka-for-real-time-event-streaming">2. Using Kafka for Real-Time Event Streaming</h3> <p>To enable real-time data processing, we implemented Apache Kafka as our central event streaming platform:</p> <ul> <li> <p><strong>Event Sourcing</strong>: Designed an event-driven architecture where all significant state changes (e.g., user clicks, purchases, inventory updates) were captured as events in Kafka topics.</p> <p><em>Example</em>: We created a <code class="language-plaintext highlighter-rouge">user-activity</code> Kafka topic to capture all user interactions. Here’s a sample event schema:</p> <div class="language-json highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"event_type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"product_view"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"user_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"12345"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"product_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"89101"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"timestamp"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2023-09-06T14:30:00Z"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"session_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"abcde12345"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"device_type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"mobile"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div> </div> </li> <li> <p><strong>Kafka Streams</strong>: Utilized Kafka Streams for real-time data processing, including aggregations and joins across multiple event streams.</p> <p><em>Example</em>: We implemented a Kafka Streams application to calculate real-time product popularity scores. It consumed events from the <code class="language-plaintext highlighter-rouge">user-activity</code> and <code class="language-plaintext highlighter-rouge">purchase</code> topics, aggregating views and purchases for each product over a sliding 1-hour window.</p> </li> <li> <p><strong>Schema Registry</strong>: Implemented Confluent Schema Registry to manage and evolve our event schemas, ensuring backward compatibility.</p> <p><em>Example</em>: When we needed to add a <code class="language-plaintext highlighter-rouge">category_id</code> field to our <code class="language-plaintext highlighter-rouge">product_view</code> event, we used Schema Registry to manage this evolution, ensuring that old consumers could still process new events without breaking.</p> </li> <li> <p><strong>Kafka Connect</strong>: Used Kafka Connect to integrate with various data sources and sinks, including our legacy databases and cloud services.</p> <p><em>Example</em>: We used Kafka Connect with the JDBC source connector to stream changes from our legacy MySQL product catalog database to a <code class="language-plaintext highlighter-rouge">product-updates</code> Kafka topic. This allowed us to keep our new microservices in sync with legacy data in real-time.</p> </li> </ul> <h3 id="3-developing-microservices-with-fastapi">3. Developing Microservices with FastAPI</h3> <p>To replace our monolithic architecture, we developed a suite of microservices using FastAPI:</p> <ul> <li> <p><strong>Service Decomposition</strong>: Broke down our monolith into smaller, domain-specific services (e.g., user management, product catalog, order processing).</p> <p><em>Example</em>: We extracted the product recommendation functionality into a separate microservice. This service consumed user activity data from Kafka and exposed an API endpoint for personalized recommendations.</p> </li> <li> <p><strong>Asynchronous Processing</strong>: Leveraged FastAPI’s asynchronous capabilities to handle high-concurrency scenarios efficiently.</p> <p><em>Example</em>: In our Order Processing service, we implemented an asynchronous endpoint for order creation:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="nd">@app.post</span><span class="p">(</span><span class="sh">"</span><span class="s">/orders</span><span class="sh">"</span><span class="p">,</span> <span class="n">response_model</span><span class="o">=</span><span class="n">OrderResponse</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">create_order</span><span class="p">(</span><span class="n">order</span><span class="p">:</span> <span class="n">OrderCreate</span><span class="p">):</span>
    <span class="n">order_id</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">create_order_async</span><span class="p">(</span><span class="n">order</span><span class="p">)</span>
    <span class="k">await</span> <span class="nf">publish_order_created_event</span><span class="p">(</span><span class="n">order_id</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">order_id</span><span class="sh">"</span><span class="p">:</span> <span class="n">order_id</span><span class="p">,</span> <span class="sh">"</span><span class="s">status</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">processing</span><span class="sh">"</span><span class="p">}</span>
</code></pre></div> </div> </li> <li> <p><strong>Data Validation</strong>: Utilized Pydantic models for request/response validation and automatic OpenAPI documentation generation.</p> <p><em>Example</em>: For our User Profile service, we defined Pydantic models for user data:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">EmailStr</span>

<span class="k">class</span> <span class="nc">UserCreate</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">username</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">email</span><span class="p">:</span> <span class="n">EmailStr</span>
    <span class="n">full_name</span><span class="p">:</span> <span class="nb">str</span>

<span class="k">class</span> <span class="nc">UserResponse</span><span class="p">(</span><span class="n">UserCreate</span><span class="p">):</span>
    <span class="nb">id</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">created_at</span><span class="p">:</span> <span class="n">datetime</span>
</code></pre></div> </div> </li> <li> <p><strong>Dependency Injection</strong>: Implemented a clean dependency injection pattern for database connections, caching, and external service clients.</p> <p><em>Example</em>: In our Product Catalog service, we used FastAPI’s dependency injection to manage database connections:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">get_db</span><span class="p">():</span>
    <span class="n">db</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">create_db_pool</span><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">db</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="k">await</span> <span class="n">db</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

<span class="nd">@app.get</span><span class="p">(</span><span class="sh">"</span><span class="s">/products/{product_id}</span><span class="sh">"</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">get_product</span><span class="p">(</span><span class="n">product_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">db</span><span class="p">:</span> <span class="n">Database</span> <span class="o">=</span> <span class="nc">Depends</span><span class="p">(</span><span class="n">get_db</span><span class="p">)):</span>
    <span class="k">return</span> <span class="k">await</span> <span class="n">db</span><span class="p">.</span><span class="nf">fetch_one</span><span class="p">(</span><span class="sh">"</span><span class="s">SELECT * FROM products WHERE id = :id</span><span class="sh">"</span><span class="p">,</span> <span class="p">{</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">:</span> <span class="n">product_id</span><span class="p">})</span>
</code></pre></div> </div> </li> </ul> <h3 id="4-orchestrating-data-pipelines-with-airflow">4. Orchestrating Data Pipelines with Airflow</h3> <p>To manage our complex data workflows, we implemented Apache Airflow:</p> <ul> <li> <p><strong>DAG Design</strong>: Created modular, reusable Directed Acyclic Graphs (DAGs) for various data processing tasks, including ETL jobs, model training, and data quality checks.</p> <p><em>Example</em>: We created a DAG for daily product sales analysis:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="nc">DAG</span><span class="p">(</span><span class="sh">'</span><span class="s">daily_product_sales_analysis</span><span class="sh">'</span><span class="p">,</span> <span class="n">schedule_interval</span><span class="o">=</span><span class="sh">'</span><span class="s">@daily</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>
    <span class="n">extract_sales_data</span> <span class="o">=</span> <span class="nc">PythonOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="sh">'</span><span class="s">extract_sales_data</span><span class="sh">'</span><span class="p">,</span>
        <span class="n">python_callable</span><span class="o">=</span><span class="n">extract_sales_data_func</span>
    <span class="p">)</span>
    <span class="n">transform_sales_data</span> <span class="o">=</span> <span class="nc">PythonOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="sh">'</span><span class="s">transform_sales_data</span><span class="sh">'</span><span class="p">,</span>
        <span class="n">python_callable</span><span class="o">=</span><span class="n">transform_sales_data_func</span>
    <span class="p">)</span>
    <span class="n">load_sales_data</span> <span class="o">=</span> <span class="nc">PythonOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="sh">'</span><span class="s">load_sales_data</span><span class="sh">'</span><span class="p">,</span>
        <span class="n">python_callable</span><span class="o">=</span><span class="n">load_sales_data_func</span>
    <span class="p">)</span>
    <span class="n">extract_sales_data</span> <span class="o">&gt;&gt;</span> <span class="n">transform_sales_data</span> <span class="o">&gt;&gt;</span> <span class="n">load_sales_data</span>
</code></pre></div> </div> </li> <li> <p><strong>Sensor Tasks</strong>: Implemented custom sensors to trigger workflows based on events in our Kafka streams or changes in our data lake.</p> <p><em>Example</em>: We created a custom Kafka sensor to trigger our product recommendation model retraining when significant user behavior changes were detected:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">UserBehaviorChangeSensor</span><span class="p">(</span><span class="n">BaseSensorOperator</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">poke</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="n">consumer</span> <span class="o">=</span> <span class="nc">KafkaConsumer</span><span class="p">(</span><span class="sh">'</span><span class="s">user-behavior-metrics</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="n">consumer</span><span class="p">.</span><span class="nf">poll</span><span class="p">(</span><span class="n">timeout_ms</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">message</span><span class="p">.</span><span class="n">value</span><span class="p">[</span><span class="sh">'</span><span class="s">behavior_change_score</span><span class="sh">'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.7</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">True</span>
        <span class="k">return</span> <span class="bp">False</span>
</code></pre></div> </div> </li> <li> <p><strong>Dynamic DAG Generation</strong>: Developed a system to dynamically generate DAGs based on configuration files, allowing business users to define new data pipelines without coding.</p> <p><em>Example</em>: We created a YAML-based configuration system for generating ETL DAGs:</p> <div class="language-yaml highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="na">dag_id</span><span class="pi">:</span> <span class="s">product_category_etl</span>
<span class="na">schedule_interval</span><span class="pi">:</span> <span class="s2">"</span><span class="s">@daily"</span>
<span class="na">tasks</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">extract_category_data</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">postgres_to_s3</span>
    <span class="na">source_table</span><span class="pi">:</span> <span class="s">product_categories</span>
    <span class="na">s3_bucket</span><span class="pi">:</span> <span class="s">raw-data</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">transform_category_data</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">spark_job</span>
    <span class="na">job_name</span><span class="pi">:</span> <span class="s">category_transformer</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">load_category_data</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">s3_to_redshift</span>
    <span class="na">s3_bucket</span><span class="pi">:</span> <span class="s">transformed-data</span>
    <span class="na">redshift_table</span><span class="pi">:</span> <span class="s">dim_product_categories</span>
</code></pre></div> </div> </li> <li> <p><strong>Monitoring and Alerting</strong>: Set up comprehensive monitoring and alerting for all Airflow tasks, integrated with our existing PagerDuty system.</p> <p><em>Example</em>: We configured Airflow to send alerts to PagerDuty when critical DAGs failed or ran longer than expected. For our daily sales reconciliation DAG, we set up an alert if the job took more than 2 hours to complete or if the final reconciliation step showed a discrepancy greater than 0.1%.</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div id="architecture-diagram"></div> </div> </div> <div class="caption"> Real Time Data Platform </div> <h3 id="5-implementing-additional-integration-technologies">5. Implementing Additional Integration Technologies</h3> <p>To further enhance our system’s capabilities and ensure comprehensive coverage of integration technologies, we implemented additional solutions:</p> <ul> <li> <p><strong>Message Queuing</strong>: While Kafka handled our event streaming needs, we implemented RabbitMQ for task queuing and point-to-point messaging scenarios.</p> <p><em>Example</em>: We used RabbitMQ for our order processing workflow. When an order is placed, a message is sent to a queue, which is then processed by our fulfillment service. This allowed us to handle spikes in order volume gracefully.</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Publishing an order to RabbitMQ
</span><span class="n">channel</span><span class="p">.</span><span class="nf">basic_publish</span><span class="p">(</span>
    <span class="n">exchange</span><span class="o">=</span><span class="sh">''</span><span class="p">,</span>
    <span class="n">routing_key</span><span class="o">=</span><span class="sh">'</span><span class="s">order_processing</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">body</span><span class="o">=</span><span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">(</span><span class="n">order_data</span><span class="p">),</span>
    <span class="n">properties</span><span class="o">=</span><span class="n">pika</span><span class="p">.</span><span class="nc">BasicProperties</span><span class="p">(</span>
        <span class="n">delivery_mode</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># make message persistent
</span>    <span class="p">))</span>
</code></pre></div> </div> </li> <li> <p><strong>Enterprise Service Bus (ESB)</strong>: We implemented an ESB using Apache Camel to handle complex integration scenarios and provide a centralized point of control for our service interactions.</p> <p><em>Example</em>: We used Camel to integrate our legacy inventory system with our new microservices architecture. Camel routes transformed the data formats and handled the communication protocols, allowing seamless integration without modifying the legacy system.</p> </li> <li> <p><strong>API Lifecycle Management</strong>: Extended our API management solution to cover the full API lifecycle, including design, testing, deployment, and retirement.</p> <p><em>Example</em>: We implemented a CI/CD pipeline specifically for APIs, which automated the process of updating API documentation, running contract tests, deploying to staging, and promoting to production upon approval.</p> </li> </ul> <p>These additional integration technologies complemented our existing architecture, providing:</p> <ul> <li>Improved resilience through decoupled, asynchronous processing</li> <li>Better interoperability between legacy and modern systems</li> <li>Enhanced governance and control over our growing API ecosystem</li> </ul> <p>The combination of these technologies with our existing Kafka, FastAPI, and Airflow implementation created a robust, flexible, and scalable integration layer that could adapt to our evolving business needs.</p> <h2 id="result">Result</h2> <p>The implementation of our new data platform led to significant improvements across various aspects of our e-commerce operations:</p> <ol> <li> <p><strong>Improved System Performance</strong>:</p> <ul> <li> <strong>Real-time processing</strong>: Reduced data processing latency from hours to <strong>milliseconds</strong> for critical workflows. <em>Example</em>: Product recommendations now update within 500ms of a user action, compared to the previous 4-hour delay.</li> <li> <strong>Scalability</strong>: Successfully handled <strong>Black Friday peak of 150,000 requests per second</strong>, a 200% increase from the previous year, with no downtime. <em>Example</em>: Our systems remained responsive even during a flash sale that generated 3x normal traffic.</li> <li> <strong>API response times</strong>: Decreased average API response time by <strong>75%</strong>, from 200ms to 50ms. <em>Example</em>: The product search API now returns results in under 100ms for 99% of queries.</li> </ul> </li> <li> <p><strong>Enhanced Development Efficiency</strong>:</p> <ul> <li> <strong>Time-to-market</strong>: Reduced new feature development time by <strong>60%</strong> due to the API-first approach and microservices architecture. <em>Example</em>: Integration of a new payment gateway was completed in 3 weeks, compared to 3 months previously.</li> <li> <strong>Code reusability</strong>: Increased code reuse by <strong>40%</strong> through well-defined API contracts and shared libraries. <em>Example</em>: Our new user authentication microservice is now used across 12 different applications.</li> <li> <strong>Testing efficiency</strong>: Improved test coverage by <strong>30%</strong> and reduced integration testing time by <strong>50%</strong> with contract testing. <em>Example</em>: Automated contract tests now catch 95% of integration issues before they reach production.</li> </ul> </li> <li> <p><strong>Business Metrics Improvement</strong>:</p> <ul> <li> <strong>Conversion rate</strong>: Increased overall conversion rate by <strong>15%</strong> due to more accurate, real-time personalization. <em>Example</em>: The “You May Also Like” section now has a 22% click-through rate, up from 15%.</li> <li> <strong>Customer engagement</strong>: Boosted average session duration by <strong>25%</strong> and pages per session by <strong>20%</strong>. <em>Example</em>: Mobile app users now spend an average of 7 minutes per session, up from 5.5 minutes.</li> <li> <strong>Revenue impact</strong>: Achieved a <strong>12% increase in average order value</strong> attributed to improved product recommendations. <em>Example</em>: Cross-sell suggestions during checkout increased attachment rate from 8% to 13%.</li> </ul> </li> <li> <p><strong>Operational Efficiency</strong>:</p> <ul> <li> <strong>Data pipeline reliability</strong>: Increased data pipeline success rate from 92% to <strong>99.9%</strong> with Airflow orchestration. <em>Example</em>: Our daily sales reconciliation process now completes successfully 29 days out of 30, up from 25.</li> <li> <strong>Incident response</strong>: Reduced mean time to detect (MTTD) for data issues by <strong>70%</strong> and mean time to resolve (MTTR) by <strong>50%</strong>. <em>Example</em>: A recent pricing discrepancy was detected and corrected within 15 minutes, compared to 2 hours previously.</li> <li> <strong>Resource utilization</strong>: Optimized cloud infrastructure costs, resulting in a <strong>25% reduction</strong> in per-transaction processing cost. <em>Example</em>: Our monthly AWS bill decreased by $50,000 despite a 30% increase in traffic.</li> </ul> </li> <li> <p><strong>Data Accessibility and Quality</strong>:</p> <ul> <li> <strong>Real-time analytics</strong>: Provided marketing and product teams with dashboards updated in real-time, leading to <strong>35% faster</strong> decision-making in campaign management. <em>Example</em>: A/B test results are now available within minutes, allowing for rapid iteration of marketing campaigns.</li> <li> <strong>Data consistency</strong>: Achieved <strong>99.99% data consistency</strong> across all systems with our unified data model and schema management. <em>Example</em>: Customer lifetime value calculations now match across all reports and systems.</li> <li> <strong>Data freshness</strong>: Improved data freshness SLAs, with <strong>95% of all data</strong> now being less than 5 minutes old. <em>Example</em>: Inventory levels in our mobile app now reflect in-store purchases within 2 minutes.</li> </ul> </li> </ol> <p>In conclusion, the implementation of our scalable, real-time data platform using API-first design, Kafka, FastAPI, and Airflow transformed our e-commerce operations. It not only solved our immediate technical challenges but also positioned us for future growth and innovation in the highly competitive e-commerce landscape.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <canvas id="performance-chart"></canvas> </div> </div> <script src="https://cdn.jsdelivr.net/npm/chart.js"></script> <script src="/assets/js/supply-chain-analytics/chart.js"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.13.10/mermaid.min.js"></script> <script>mermaid.initialize({startOnLoad:!0}),document.addEventListener("DOMContentLoaded",function(){var n='\n    graph TB\n        subgraph "API-First Design Process"\n            A[API Design Workshops] --> B[OpenAPI Specification]\n            B --> C[Contract Testing]\n        end\n\n        subgraph "Client Applications"\n            D[Web App]\n            E[Mobile App]\n            F[Partner Systems]\n        end\n\n        subgraph "API Layer"\n            G[Kong API Gateway]\n            H[Authentication & Authorization]\n        end\n\n        subgraph "Microservices Layer"\n            I[User Service]\n            J[Product Catalog Service]\n            K[Order Service]\n            L[Recommendation Service]\n            M[Inventory Service]\n        end\n\n        subgraph "Event Streaming Layer"\n            N[Kafka Cluster]\n            O[Kafka Connect]\n            P[Kafka Streams]\n            Q[Schema Registry]\n        end\n\n        subgraph "Data Processing Layer"\n            R[Airflow]\n            S[ETL Jobs]\n            T[Machine Learning Pipeline]\n        end\n\n        subgraph "Data Storage Layer"\n            U[Operational Databases]\n            V[Data Warehouse]\n            W[Data Lake]\n        end\n\n        subgraph "Analytics & Monitoring"\n            X[Real-time Dashboards]\n            Y[Monitoring & Alerting]\n        end\n\n        %% Client to API Gateway\n        D --> G\n        E --> G\n        F --> G\n\n        %% API Gateway to Microservices\n        G --> H\n        H --> I\n        H --> J\n        H --> K\n        H --> L\n        H --> M\n\n        %% Microservices to Kafka\n        I --> N\n        J --> N\n        K --> N\n        L --> N\n        M --> N\n\n        %% Kafka to Data Processing\n        N --> O\n        N --> P\n        N --> Q\n        O --> R\n        P --> R\n\n        %% Data Processing to Storage\n        R --> S\n        R --> T\n        S --> U\n        S --> V\n        S --> W\n        T --> W\n\n        %% Analytics & Monitoring\n        N --> X\n        R --> Y\n        G --> Y\n\n        %% API-First Design influence\n        C -.-> G\n        C -.-> I\n        C -.-> J\n        C -.-> K\n        C -.-> L\n        C -.-> M\n\n        %% Data flow to Recommendation Service\n        N --> L\n        W --> L\n\n        %% Airflow orchestration\n        R --> N\n        R --> U\n        R --> V\n        R --> W\n\n        classDef microservice fill:#f9f,stroke:#333,stroke-width:2px;\n        class I,J,K,L,M microservice;\n        classDef kafka fill:#afd,stroke:#333,stroke-width:2px;\n        class N,O,P,Q kafka;\n        classDef storage fill:#ffa,stroke:#333,stroke-width:2px;\n        class U,V,W storage;\n    ',e=function(n){document.getElementById("architecture-diagram").innerHTML=n};mermaid.render("mermaid-diagram",n,e)});</script> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Shubham Nagar. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"projects",description:"A growing collection of my data projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-migrating-from-tableau-to-power-bi-a-comprehensive-guide",title:'Migrating from Tableau to Power BI: A Comprehensive Guide <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/mind-and-machine/migrating-from-tableau-to-power-bi-a-comprehensive-guide-b6e4929e1ea3?source=rss-5ad90eb46828------2","_blank")}},{id:"post-discovering-anomalies-with-mad-the-secret-sauce-for-accurate-data-analysis",title:'Discovering Anomalies with MAD: The Secret Sauce for Accurate Data Analysis <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/mind-and-machine/discovering-anomalies-with-mad-the-secret-sauce-for-accurate-data-analysis-ed1c7909e2bf?source=rss-5ad90eb46828------2","_blank")}},{id:"post-harnessing-agentic-rag-and-graph-based-metadata-filtering-for-enhanced-information-retrieval",title:'Harnessing Agentic RAG and Graph-Based Metadata Filtering for Enhanced Information Retrieval <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/mind-and-machine/harnessing-agentic-rag-and-graph-based-metadata-filtering-for-enhanced-information-retrieval-5e4fc88dcdc0?source=rss-5ad90eb46828------2","_blank")}},{id:"post-the-critical-role-of-red-teaming-in-ai-development",title:'The Critical Role of Red Teaming in AI Development <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/mind-and-machine/the-critical-role-of-red-teaming-in-ai-development-8a1b393cfc51?source=rss-5ad90eb46828------2","_blank")}},{id:"post-who-am-i",title:'Who am I? <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@mail.shubhamnagar/who-am-i-f6810254e1ba?source=rss-5ad90eb46828------2","_blank")}},{id:"post-trust-or-distrust-bridging-ai-and-blockchain",title:'Trust or Distrust: Bridging AI and Blockchain <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/mind-and-machine/trust-or-distrust-bridging-ai-and-blockchain-659c5760ef6?source=rss-5ad90eb46828------2","_blank")}},{id:"post-ai-technologies-in-the-new-era-catalysts-or-hindrances-for-the-rpa-industry",title:'AI Technologies in the New Era: Catalysts or Hindrances for the RPA Industry?... <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@mail.shubhamnagar/ai-technologies-in-the-new-era-catalysts-or-hindrances-for-the-rpa-industry-e3c083950ced?source=rss-5ad90eb46828------2","_blank")}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-advanced-supply-chain-analytics-platform-optimizing-global-logistics-with-real-time-data",title:"Advanced Supply Chain Analytics Platform - Optimizing Global Logistics with Real-time Data",description:"Developed a cutting-edge supply chain analytics platform integrating real-time IoT data, predictive analytics, and interactive visualizations to optimize global logistics operations.",section:"Projects",handler:()=>{window.location.href="/projects/advanced-data-visualization-project/"}},{id:"projects-apache-kafka",title:"Apache kafka",description:"Kafka How and Why",section:"Projects",handler:()=>{window.location.href="/projects/apache-kafka/"}},{id:"projects-apache-spark",title:"Apache spark",description:"Spark How and Why",section:"Projects",handler:()=>{window.location.href="/projects/apche-spark/"}},{id:"projects-aws-sqs",title:"AWS SQS",description:"SQS How and Why",section:"Projects",handler:()=>{window.location.href="/projects/aws-sqs/"}},{id:"projects-data-integration-project-telco-colibra",title:"Data Integration Project (Telco) - Colibra",description:"Implemented a comprehensive data integration solution for a global logistics company using Collibra and Mega HOPEX, resulting in improved operational efficiency and data-driven decision making.",section:"Projects",handler:()=>{window.location.href="/projects/colibra-data-integration-telco/"}},{id:"projects-data-integration-project-supply-chain-colibra",title:"Data Integration Project (Supply Chain) - Colibra",description:"Implemented a comprehensive data integration solution for a global logistics company using Collibra and Mega HOPEX, resulting in improved operational efficiency and data-driven decision making.",section:"Projects",handler:()=>{window.location.href="/projects/colibra-data-integration/"}},{id:"projects-enhancing-metadata-management-using-collibra-business-glossary",title:"Enhancing Metadata Management Using Collibra Business Glossary",description:"Implemented a robust metadata management system using Collibra and integrated it with Mega HOPEX for improved data governance and consistency across the organization.",section:"Projects",handler:()=>{window.location.href="/projects/collibra-metadata-case-study/"}},{id:"projects-how-collibra-and-leanix-complement-each-other-in-telecom",title:"How Collibra and LeanIX Complement Each Other in Telecom",description:"How Collibra and LeanIX Complement Each Other in Telecom",section:"Projects",handler:()=>{window.location.href="/projects/collibra-plus-leanix/"}},{id:"projects-collibra",title:"Collibra",description:"Collibra Explained Simply",section:"Projects",handler:()=>{window.location.href="/projects/collibra/"}},{id:"projects-comprehensive-data-architecture-strategy-for-global-supply-chain-and-logistics",title:"Comprehensive Data Architecture Strategy for Global Supply Chain and Logistics",description:"Designed and implemented a comprehensive data architecture strategy for a global supply chain and logistics customer, improving data accessibility by 35% and streamlining reporting processes across their international operations.",section:"Projects",handler:()=>{window.location.href="/projects/comprehensive-data-architecture-strategy/"}},{id:"projects-data-lake-to-data-fabric-modernizing-data-architecture-for-a-multinational-corporation",title:"Data Lake to Data Fabric - Modernizing Data Architecture for a Multinational Corporation...",description:"Transitioning from a traditional data lake to a data fabric approach, improving data accessibility, governance, and analytics agility",section:"Projects",handler:()=>{window.location.href="/projects/data-fabric-case-study/"}},{id:"projects-revolutionizing-supply-chain-data-governance-at-globallogistics",title:"Revolutionizing Supply Chain Data Governance at GlobalLogistics",description:"Implementing enterprise-wide data standards and quality measures to streamline global logistics operations",section:"Projects",handler:()=>{window.location.href="/projects/data-governance-collibra/"}},{id:"projects-data-pipeline-management-with-apache-spark-and-talend",title:"Data Pipeline Management with Apache Spark and Talend",description:"Designed and managed data pipelines for data validation, transformation, and cleaning using Apache Spark and Talend, ensuring the highest quality and reliability of data across the organization.",section:"Projects",handler:()=>{window.location.href="/projects/data-pipeline-management/"}},{id:"projects-data-modeling-for-e-commerce-reducing-redundancy-and-improving-customer-insights",title:"Data Modeling for E-commerce - Reducing Redundancy and Improving Customer Insights",description:"A comprehensive overhaul of an e-commerce platform's data model, resulting in 40% reduced data redundancy and 25% improved customer segmentation accuracy.",section:"Projects",handler:()=>{window.location.href="/projects/ecommerce-data-modeling/"}},{id:"projects-implementing-data-governance-a-journey-from-silos-to-standardization",title:"Implementing Data Governance - A Journey from Silos to Standardization",description:"Transforming organizational data management through strategic governance initiatives",section:"Projects",handler:()=>{window.location.href="/projects/implementing-data-governance/"}},{id:"projects-sap-leanix",title:"SAP LeanIX",description:"LeanIX Explained Simply",section:"Projects",handler:()=>{window.location.href="/projects/leanix/"}},{id:"projects-modernizing-supply-chain-integration-at-globallogistics",title:"Modernizing Supply Chain Integration at GlobalLogistics",description:"Implementing cutting-edge integration technologies to streamline global supply chain operations",section:"Projects",handler:()=>{window.location.href="/projects/modern-supply-chain/"}},{id:"projects-modernizing-telco-integration-at-telenet",title:"Modernizing Telco Integration at Telenet",description:"Proposed implementation of cutting-edge integration technologies to streamline telecom operations",section:"Projects",handler:()=>{window.location.href="/projects/modern-telco-integration/"}},{id:"projects-mulesoft",title:"Mulesoft",description:"A deep dive into how MuleSoft Anypoint Platform transformed global supply chain operations",section:"Projects",handler:()=>{window.location.href="/projects/mulesoft/"}},{id:"projects-scalable-real-time-data-platform-for-e-commerce",title:"Scalable Real-Time Data Platform for E-Commerce",description:"Implemented a high-performance data platform using API-first design, Kafka, FastAPI, and Airflow for an e-commerce giant, enabling real-time personalization and handling over 100,000 requests per second.",section:"Projects",handler:()=>{window.location.href="/projects/realtime-data-platform/"}},{id:"projects-enhancing-regulatory-compliance-and-data-governance-in-global-supply-chain-operations",title:"Enhancing Regulatory Compliance and Data Governance in Global Supply Chain Operations",description:"A case study on implementing Collibra and Mega HOPEX to streamline compliance and data management for GlobalLogistics",section:"Projects",handler:()=>{window.location.href="/projects/regulatory-compliance/"}},{id:"projects-snowflake-data-warehousing-project-optimizing-analytics-for-scale",title:"Snowflake Data Warehousing Project - Optimizing Analytics for Scale",description:"Implemented a Snowflake data warehouse solution, achieving a 40% reduction in query times and enabling real-time analytics across diverse data sources for a rapidly growing e-commerce company.",section:"Projects",handler:()=>{window.location.href="/projects/snowflake-project/"}},{id:"projects-sqs-vs-kafka",title:"SQS vs Kafka",description:"Complementing Kafka with SQS",section:"Projects",handler:()=>{window.location.href="/projects/sqs-kafka/"}},{id:"projects-streamlining-supply-chain-data-architecture-at-globallogistics",title:"Streamlining Supply Chain Data Architecture at GlobalLogistics",description:"A comprehensive data integration project that revolutionized GlobalLogistics' supply chain operations",section:"Projects",handler:()=>{window.location.href="/projects/streamlining-supply-chain-architecture/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6D%61%69%6C.%73%68%75%62%68%61%6D%6E%61%67%61%72@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/shubham184","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/shubham-nagar-222497151","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>